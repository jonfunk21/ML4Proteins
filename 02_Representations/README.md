# Introduction

This lecture provides an introduction to protein representations and representation learning through latent variable models (LVMs). 
We are going to cover:

- **Simple representations**:
    a. Revision of OHE and introduction of **BLOSUM** encodings and **biochemical** encodings.  
    b. Introduction to **latent variable models**.

### What You'll Learn

1. **Simple representations**  
   - What are protein representations, and why do they matter?  
   - What are inductive biases?  
   - What should be paid attention to when constructing representations?  

2. **Latent variable models and representation learning**  
   - What is representation learning?  
   - How does it differ from simple protein representations?  
   - How to train an LVMâ€”specifically, a Variational Autoencoder.  
   - Controlling inductive biases in LVM training and architecture.  

### How We'll Proceed

We will begin by revisiting the exercises from day one using new tools. We will explore representation learning 
methods and study their effects on model performance.  
Next, we will train latent variable models and examine the resulting representations and their influence on model performance.  
Finally, we will learn how **ProteusAI** can be used to accomplish these tasks.

### Bonus Content

For those interested, we also provide a Jupyter notebook on protein language models (pLMs) and structural representation models, 
which are among the most advanced and powerful representation learning algorithms. If you are following the course 
[27666 AI-guided Protein Science: From Design to Engineering](https://kurser.dtu.dk/course/2024-2025/27666?menulanguage=en) 
at the Technical University of Denmark (DTU), you will learn more about them in later lectures.
