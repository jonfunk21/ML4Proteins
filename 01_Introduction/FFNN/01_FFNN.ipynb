{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef87f5c4-5bfc-4fac-afc1-0d1ed20f6550",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning for Protein Engineering - Feed forward neural networks (FFNN)\n",
    "---\n",
    "\n",
    "## Preface\n",
    "In this section, we will delve into the fundamentals of machine learning (ML) using PyTorch, specifically applied to the domain of protein engineering. If you are new to ML, it is highly recommended to start by reading the theoretical background in the README.md file within this folder.\n",
    "\n",
    "As you navigate through the Jupyter Notebook, you will encounter \"Cells\" containing code along with accompanying explanatory text and questions. While reviewing both the code and text, aim to comprehend the underlying processes. If you encounter difficulties understanding a particular cell, don't hesitate to consult your favorite LLM chatbot (such as ChatGPT or Bard) for a detailed explanation.\n",
    "\n",
    "The primary objective of this course is to familiarize you with the appearance of ML code and provide a broad overview of the structure of ML projects. Rather than attempting to meticulously memorize the code, which has arguably become a less vital skill in the era of ChatGPT, especially at the introductory level, focus on grasping the core concepts. Challenge the validity of assumptions and gain an understanding of the general framework of ML as it applies to protein engineering.\n",
    "\n",
    "Enjoy reading and learning! :)\n",
    "\n",
    "## The GB1 dataset\n",
    "In this section, we will investigate the mutational effects on the binding affinity of the IgG-binding domain of protein G (GB1). The data we are utilizing has been quantitatively derived, determining the effects of single mutations on the structural stability (ΔΔGU) of GB1, as detailed in the study by Anders Olson et al. [Read more](https://www.sciencedirect.com/science/article/pii/S0960982214012688?via%3Dihub).\n",
    "\n",
    "It's important to note that in the original study, all mutational effects of both single and double mutants were examined. However, in this context, we are solely focusing on the single mutants, making it convenient for you to run the analysis on your local machines.\n",
    "\n",
    "In the first code cells we will load the data and transform it into a useful (numerical) representation so we can use it for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73797aa6-76bc-4fd4-978f-db879b9b8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wt_seq = \"MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\"\n",
    "\n",
    "# read in mutant data\n",
    "df = pd.read_csv('GB1_single_mutants.csv')\n",
    "\n",
    "# check it out\n",
    "display(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f3766-7a73-4b43-958c-454220845000",
   "metadata": {},
   "source": [
    "## Encoding Protein Sequences: From Amino Acids to Numerical Data\n",
    "---\n",
    "When dealing with sequences in machine learning, we need to transform them into a numerical format before using them in our algorithms. We aim to convert an amino acid sequence \\\\(\\mathbf{S}\\\\) into a numerical representation \\\\(\\mathbf{X}\\\\) that can be interpreted by machine learning models. This conversion can be represented as a mapping from \\\\(\\mathbf{S}\\\\) to \\\\(\\mathbf{X}\\\\):\n",
    "\n",
    "\\\\[S \\rightarrow \\mathbf{X}\\\\]\n",
    "\n",
    "One common method for this mapping is _one-hot encoding_ (OHE). One-hot encoding involves assigning each unique character \\\\(s_i\\\\) in the sequence \\\\(\\mathbf{S}\\\\) to a vector \\\\(\\vec{x_i}\\\\), which is \\\\(0\\\\) in every position except for a single position where the vector has the value \\\\(1\\\\).\n",
    "\n",
    "\\\\[\n",
    "\\text{OneHot}(s_i) = \\begin{cases}\n",
    "     1, & \\text{if } s_i \\text{ is the amino acid of interest} \\\\\n",
    "     0, & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "\\\\]\n",
    "\n",
    "Here, \\\\(\\vec{s_i}\\\\) represents an amino acid at position \\\\(i\\\\) in the sequence \\\\(\\mathbf{S}\\\\). One-hot encoding produces a binary vector \\\\(\\vec{x}_i\\\\) for each amino acid \\\\(\\vec{s_i}\\\\), such that:\n",
    "\n",
    "\\\\[\\vec{x}_i = [0, 0, \\ldots, 1, \\ldots, 0]\\\\]\n",
    "\n",
    "where the '\\\\(1\\\\)' occurs at the position corresponding to the amino acid in consideration, and all other elements are '\\\\(0\\\\'. \n",
    "\n",
    "This results in a numerical matrix \\\\(\\mathbf{X}\\\\) representing \\\\(\\mathbf{S}\\\\), where each *row* corresponds to an amino acid position, and each column represents a specific amino acid type. Thus:\n",
    "\n",
    "\\\\[\\mathbf{X}_{ij} = \\text{OneHot}(s_i)_j\\\\]\n",
    "\n",
    "### Questions\n",
    "1. Why do you think one-hot encoding (OHE) is popular for representing sequences?\n",
    "2. Why don't we simply encode each amino acid with a unique integer?\n",
    "3. Can you think of any potential downsides to OHE? (Tip: Look at the visualizations below)\n",
    "4. Analyze the following code segments, and discuss what is happening.\n",
    "\n",
    "### OHE Implementation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74024769-82bd-44d9-827c-969f13b9ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(seq):\n",
    "    # Dictionary of standard amino acids\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    aa_to_int = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "    # Initialize the one-hot encoded matrix\n",
    "    one_hot = np.zeros((len(seq), len(amino_acids)), dtype=int)\n",
    "\n",
    "    # Fill the matrix\n",
    "    for i, aa in enumerate(seq):\n",
    "        if aa in aa_to_int:\n",
    "            one_hot[i, aa_to_int[aa]] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f87e12-8be2-4dd7-a5ed-80b6101707f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_one_hot(encoded_seq, seq):\n",
    "    # Create a heatmap from the one-hot encoded matrix\n",
    "    plt.figure(figsize=(10, len(encoded_seq) / 2))\n",
    "    sns.heatmap(encoded_seq, annot=True, cbar=False, cmap='rocket',\n",
    "                xticklabels=list('ACDEFGHIKLMNPQRSTVWY'), yticklabels=list(seq))\n",
    "    plt.xlabel('Amino Acids')\n",
    "    plt.ylabel('Sequence Position')\n",
    "    plt.title('One-Hot Encoding of Amino Acid Sequence')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "seq = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "encoded_seq = one_hot_encode(seq)\n",
    "visualize_one_hot(encoded_seq, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5a1f3-593d-4db2-b36e-7b37e03f68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sequences(sequences):\n",
    "    num_seqs = len(sequences)\n",
    "    cols = min(num_seqs, 5)\n",
    "    rows = (num_seqs - 1) // 5 + 1\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 4, rows * 2))\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        encoded_seq = one_hot_encode(seq)\n",
    "        ax = axs[i // 5, i % 5] if rows > 1 else axs[i]\n",
    "        \n",
    "        sns.heatmap(encoded_seq, annot=True, cbar=False, cmap='rocket', ax=ax,\n",
    "                    xticklabels=list('ACDEFGHIKLMNPQRSTVWY'), yticklabels=list(seq))\n",
    "        ax.set_title(f'Sequence {i+1}')\n",
    "        ax.set_xlabel('Amino Acids')\n",
    "        ax.set_ylabel('Position')\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, rows * cols):\n",
    "        fig.delaxes(axs.flatten()[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add0a12-7301-4252-bb7d-f050e1963786",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [seq[0:6] for seq in df.sequence.to_list()]\n",
    "sequences = sequences[:10]\n",
    "\n",
    "visualize_sequences(sequences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a962a-7864-4096-ad90-1ec6b45605a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding all sequences\n",
    "df['encoded'] = df['sequence'].apply(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b102f-81c7-47d8-8b62-95bcd0cfa80a",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "---\n",
    "In the process of data splitting, we divide our dataset into three distinct sets, each serving a specific purpose:\n",
    "\n",
    "1. **Training Set**: This set forms the cornerstone of model training. It's the portion of data used to teach the model how to make predictions or classifications. The model learns from the patterns and relationships within this set.\n",
    "\n",
    "2. **Testing Set**: The testing set remains untouched during training and is reserved for evaluating the model's performance post-training, akin to a final exam. By assessing the model on this independent dataset, we can measure how well it generalizes to unseen data. This helps us gauge the model's real-world applicability.\n",
    "\n",
    "3. **Validation Set**: Think of the validation set as a quality control mechanism. It aids in fine-tuning our model during training without dipping into the testing set, much like a practice test before the final exam. This set is used for model validation and hyperparameter tuning. We make adjustments to our model based on its performance on the validation set to ensure optimal results.\n",
    "\n",
    "Data splitting is essential to ensure that our model isn't merely memorizing correct answers from the training data. By training on one portion of the data and testing on another, we can gauge how well the model is likely to perform on new, unseen data. This process helps us avoid overfitting, where the model becomes too specialized on the training data and performs poorly on new examples.\n",
    "\n",
    "### Key Benefits of Data Splitting:\n",
    "\n",
    "1. **Model Assessment**: Data splitting empowers us to assess how effectively our model performs on unseen data (the testing set) and make any necessary adjustments. It provides an objective measure of the model's real-world performance.\n",
    "\n",
    "2. **Hyperparameter Tuning**: The validation set plays a pivotal role in fine-tuning model hyperparameters, ensuring that our model is optimized for the specific task. By adjusting hyperparameters like learning rates or network architectures based on validation set performance, we can achieve better model performance.\n",
    "\n",
    "### Question\n",
    "1. What would be the consequences if we chose not to split the data during training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb3b5e-b7d6-4cdd-9c92-0b0afcccf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a414a7-5da4-4ef9-bc66-f5988cc251c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, fitness_values):\n",
    "        self.sequences = sequences\n",
    "        self.fitness_values = fitness_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        fitness = self.fitness_values[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(fitness, dtype=torch.float32)\n",
    "\n",
    "# Prepare your dataset\n",
    "X = np.stack(df['encoded'].values)\n",
    "y = df['fitness'].values\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# check out shape\n",
    "print(f'Training set has {X_train.shape[0]} observations.')\n",
    "print(f'Validation set has {X_val.shape[0]} observations.')\n",
    "print(f'Training set has {X_test.shape[0]} observations.')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad843d0b-2a24-489b-8143-9f1326d8d5bf",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "---\n",
    "In the following we are going to define the feed-forward neural netwrok (FFNN) architecture used for our regression task. Notice that the network is designed in a flexible way, such that the hidden layers can be provided as a list of integer values to determine the number of neurons per hidden layer (e.g. [10, 10]). \n",
    "\n",
    "### Questions\n",
    "1. What is the activation function used in this example?\n",
    "2. What is the input dimension to the neural network (how many neurons are in the first layer)?\n",
    "3. What is the output dimension of the FFNN (how many neurons are in the final layer)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21b490-52d5-4239-ae2e-6513387cdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers):\n",
    "        super(FFNN, self).__init__()\n",
    "\n",
    "        # Create a list to hold all layers\n",
    "        layers = []\n",
    "\n",
    "        # Add the input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "\n",
    "        # Register all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43467c-4b19-460e-b5a3-fe6c58398939",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "---\n",
    "Training an Artificial Neural Network (ANN) is an iterative process, often involving a series of steps that are repeated for a number of epochs. An epoch is one complete pass through the entire training dataset. The Python function train you provided encapsulates this process, commonly known as the training loop. It systematically adjusts the model's parameters (weights and biases) based on the given data. Let's dissect the key components and their rationale:\n",
    "\n",
    "### The Training Loop Structure\n",
    "**Initialization of Loss Lists**: `train_losses` and `val_losses` are initialized to track the loss (a measure of prediction error) during training and validation phases across epochs.\n",
    "\n",
    "**Iterating Over Epochs**: The loop for epoch in `range(num_epochs)` iterates over the dataset multiple times. Each epoch represents a complete pass through the training data.\n",
    "\n",
    "### The Training Phase\n",
    "Model in Training Mode: `model.train()` sets the model to training mode, enabling features like dropout and batch normalization which are specific to training.\n",
    "\n",
    "**Batch Processing**: The training data is usually divided into batches (`sequences`, `fitness` in `train_loader`). Batches allow the model to update weights incrementally, improving convergence and efficiency.\n",
    "\n",
    "**Forward Pass**: The model computes predictions `outputs = model(sequences)` for each batch.\n",
    "\n",
    "**Calculating Loss**: The `criterion(outputs, fitness.unsqueeze(1))` calculates the loss, a measure of how far the model's predictions are from the actual values.\n",
    "\n",
    "### Backward Pass and Optimization:\n",
    "\n",
    "`loss.backward()` computes the gradient of the loss with respect to the model parameters.\n",
    "`optimizer.step()` updates the weights based on these gradients.\n",
    "`optimizer.zero_grad()` resets gradients to zero for the next batch.\n",
    "**Tracking Training Loss**: The average loss per epoch is calculated and stored for later visualization and analysis.\n",
    "\n",
    "### The Validation Phase\n",
    "**Model in Evaluation Mode**: `model.eval()` sets the model to evaluation mode, disabling training-specific features like dropout.\n",
    "\n",
    "**No Gradient Calculation**: with `torch.no_grad()` ensures that gradients are not computed during validation, reducing memory usage and speeding up computations.\n",
    "\n",
    "**Validation Loss Calculation**: Similar to the training phase, but no weights are updated. It's used to assess the model's performance on unseen data.\n",
    "\n",
    "## Visualizing Losses\n",
    "**Plotting Losses**: Using Matplotlib, the function plots the training and validation losses over epochs. This visualization is crucial for understanding the model's learning process and convergence behavior.\n",
    "The rationale behind this structured approach is to systematically optimize the model's ability to make accurate predictions. By training over multiple epochs, the network gradually improves its understanding of the complex relationships within the data. The validation phase is critical for ensuring that the model is not just memorizing the training data but is capable of generalizing to new, unseen data, a concept known as overfitting. This iterative process of training and validation is key to developing robust neural network models in fields like protein engineering, where accurate and generalizable models are essential.\n",
    "\n",
    "## Questions\n",
    "1. Describe the training loop. What happens in each segment?\n",
    "2. What is the purpose of the validation phase?\n",
    "3. What differentiates the validation phase from the training phase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ef636-d583-47c5-a6fa-e92bc1a47662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # Initialization\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for sequences, fitness in train_loader:\n",
    "            if len(sequences.shape) == 3:\n",
    "                sequences = sequences.view(sequences.size(0), -1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, fitness.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for sequences, fitness in val_loader:\n",
    "                    sequences = sequences.view(sequences.size(0), -1)\n",
    "                    outputs = model(sequences)\n",
    "                    loss = criterion(outputs, fitness.unsqueeze(1))\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Check if this model has the best validation loss so far\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "    # Plotting using Matplotlib\n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(val_losses, label='Validation loss')\n",
    "    \n",
    "    # Add a vertical dotted line at the epoch when the best model is saved\n",
    "    if best_model is not None:\n",
    "        best_epoch = val_losses.index(best_val_loss) + 1\n",
    "        plt.axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the best model state_dict\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6cfa8-1cb2-439d-a918-fa67ea5c756b",
   "metadata": {},
   "source": [
    "## A closer look: Hidden Layers and Optimizer\n",
    "\n",
    "In the following section the model is actually trained. Play arround with the number of hidden layers and neurons per layer and try other optimizers. What do you notice?\n",
    "\n",
    "1. What happens if you increase/decrease the number of neurons? You can try \"extreme\" examples, e.g. 1 neuron or 1000 neurons.\n",
    "2. What effect does the optimizer have. Learn more about different optimizers [here](https://pytorch.org/docs/stable/optim.html).\n",
    "4. What do you observe when testing the model on the test set?\n",
    "5. How does the batch size affect learning?\n",
    "6. What happens if you don't shuffle during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249abc0-0239-4cf0-8c0a-395cd37e905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model Architecture\n",
    "input_size = X_train.shape[1] * X_train.shape[2]\n",
    "model = FFNN(input_size, hidden_layers=[10, 10])\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "best_model = train(model, train_loader, val_loader, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92f6e8-d413-4904-b62c-f4dd250d8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score  # Import R-squared metric\n",
    "\n",
    "# Compute true vs. predicted values using the test loader\n",
    "best_model.eval()\n",
    "true_values = []\n",
    "predicted_values = []\n",
    "with torch.no_grad():\n",
    "    for sequences, fitness in test_loader:\n",
    "        sequences = sequences.view(sequences.size(0), -1)\n",
    "        outputs = best_model(sequences)\n",
    "        true_values.extend(fitness.numpy())\n",
    "        predicted_values.extend(outputs.squeeze(1).numpy())\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(true_values, predicted_values)\n",
    "\n",
    "# Calculate R-squared (R²) value\n",
    "r_squared = r2_score(true_values, predicted_values)\n",
    "\n",
    "# Plot a scatter plot of true vs. predicted values with diagonal line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(true_values, predicted_values, alpha=0.5)\n",
    "plt.plot(np.arange(min(true_values), max(true_values), 0.1), \n",
    "         np.arange(min(true_values), max(true_values), 0.1), \n",
    "         linestyle='--', color='grey')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'True Values vs. Predicted Values (Test Set)\\n'\n",
    "          f'Pearson Correlation: {pearson_corr:.2f}, R-squared (R²): {r_squared:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abe104-67fc-4b26-828e-fe7f384477d3",
   "metadata": {},
   "source": [
    "## Data Transformations in Neural Networks: Principal Component Analysis (PCA)\n",
    "---\n",
    "In the realm of machine learning, and particularly in neural networks, preprocessing and transforming data is a crucial step. This is especially true in fields like biotechnology, where datasets can be vast and complex. One common challenge is dealing with sparse data, often arising from techniques like One-Hot Encoding (OHE) in protein sequence analysis. Principal Component Analysis (PCA) is a powerful tool for transforming such data, reducing sparsity, and extracting meaningful features.\n",
    "\n",
    "### Understanding Principal Component Analysis (PCA)\n",
    "\n",
    "The Essence of PCA: PCA is a statistical technique that simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the original variables into a new set of variables, the principal components, which are uncorrelated and which account for the maximum amount of variance in the data.\n",
    "\n",
    "**Mathematical Framework**: Mathematically, PCA involves an eigenvalue decomposition of the data covariance matrix or singular value decomposition (SVD) of the data matrix. If we denote our data matrix as $X \\in \\mathbb{R}^{n \\times m}$ (where $n$ is the number of samples and $m$ is the number of features), PCA seeks to find a transformation matrix $W$ that projects $X$ onto a lower-dimensional space. The principal components are then given by:\n",
    "\n",
    "$PC=XW$\n",
    "\n",
    "Here, the columns of $W$ are the eigenvectors of the covariance matrix of $X$, and they are ordered by the corresponding eigenvalues in descending order. The first few columns of $W$ (corresponding to the largest eigenvalues) are used to reduce the dimensionality of the data.\n",
    "\n",
    "### PCA in the Context of Protein Engineering\n",
    "Reducing Sparsity: In protein sequence analysis, where data is often encoded as sparse binary vectors (like in OHE), PCA can be used to reduce the dimensionality and sparsity. This transformation results in denser, continuous features that might capture more meaningful biological patterns with less computational cost.\n",
    "\n",
    "**Enhancing Feature Extraction**: By focusing on the principal components, PCA emphasizes the features in the data that contribute most to its variance. This can be particularly useful in identifying the most significant patterns in protein sequences, aiding in more effective training of neural networks.\n",
    "\n",
    "**Visualizing High-Dimensional Data**: PCA is also a valuable tool for visualization. By reducing data to two or three principal components, it allows for the visual exploration of complex datasets, providing insights that might not be apparent in higher dimensions.\n",
    "\n",
    "### Practical Considerations\n",
    "Choosing the Number of Components: A key decision in PCA is determining how many principal components to keep. This often involves a trade-off between reducing dimensionality and retaining enough information. Methods like the scree plot or explained variance ratio can guide this choice.\n",
    "\n",
    "**Preprocessing**: Standardizing the data (to have zero mean and unit variance) is often a prerequisite for PCA, ensuring that the principal components reflect the correlations in the data rather than the scale of the variables.\n",
    "\n",
    "**Integration with Neural Networks**: Post-PCA, the transformed data can be fed into neural networks. This can lead to faster training times and potentially better performance, as the network doesn't have to learn patterns from sparse, high-dimensional data.\n",
    "\n",
    "In summary, PCA is a transformative tool in the preprocessing pipeline for neural network models, particularly in protein engineering. It not only tackles the challenge of sparsity and high dimensionality but also enhances the model's ability to learn significant biological patterns, making it a valuable technique in the data scientist's toolkit.\n",
    "\n",
    "### Questions\n",
    "---\n",
    "1. What is the effect of dimensionality reduction via PCA on the model performance?\n",
    "2. How do different number of components (`n_components`) affect the model performance?\n",
    "3. Describe the differences between the transformed input data and the OHE.\n",
    "4. What do you observe when testing the model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58f6ff-f6a6-443d-acab-8d34c73eafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Number of components\n",
    "n_components = 60\n",
    "\n",
    "# Prepare your dataset\n",
    "X = np.stack(df['encoded'].values)\n",
    "y = df['fitness'].values\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reshape the data to 2D format if needed\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Create PCA models for each subset\n",
    "pca = PCA(n_components=n_components).fit(X_train_flat)\n",
    "\n",
    "# Transform the data using PCA\n",
    "X_train_pca = pca.transform(X_train_flat)\n",
    "X_val_pca = pca.transform(X_val_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_pca = SequenceDataset(X_train_pca, y_train)\n",
    "val_dataset_pca = SequenceDataset(X_val_pca, y_val)\n",
    "test_dataset_pca = SequenceDataset(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a84f8-6212-42b9-a98b-53dc5a608e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Create DataLoaders\n",
    "train_loader_pca = DataLoader(train_dataset_pca, batch_size=batch_size, shuffle=True)\n",
    "val_loader_pca = DataLoader(val_dataset_pca, batch_size=batch_size, shuffle=False)\n",
    "test_loader_pca = DataLoader(test_dataset_pca, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model Architecture\n",
    "input_size = X_train_pca.shape[1]\n",
    "model = FFNN(input_size, hidden_layers=[10, 10])\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "best_model_pca = train(model, train_loader_pca, val_loader_pca, criterion, optimizer, num_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04bbd7-7d75-49ef-829b-548b64a2de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true vs. predicted values using the test loader\n",
    "best_model_pca.eval()\n",
    "true_values = []\n",
    "predicted_values = []\n",
    "with torch.no_grad():\n",
    "    for sequences, fitness in test_loader_pca:\n",
    "        sequences = sequences.view(sequences.size(0), -1)\n",
    "        outputs = best_model_pca(sequences)\n",
    "        true_values.extend(fitness.numpy())\n",
    "        predicted_values.extend(outputs.squeeze(1).numpy())\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "pearson_corr, _ = pearsonr(true_values, predicted_values)\n",
    "\n",
    "# Calculate R-squared (R²) value\n",
    "r_squared = r2_score(true_values, predicted_values)\n",
    "\n",
    "# Plot a scatter plot of true vs. predicted values with diagonal line\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(true_values, predicted_values, alpha=0.5)\n",
    "plt.plot(np.arange(min(true_values), max(true_values), 0.1), \n",
    "         np.arange(min(true_values), max(true_values), 0.1), \n",
    "         linestyle='--', color='grey')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'True Values vs. Predicted Values (Test Set)\\n'\n",
    "          f'Pearson Correlation: {pearson_corr:.2f}, R-squared (R²): {r_squared:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a178aa-ccc2-4bad-99f0-8c46e79525c6",
   "metadata": {},
   "source": [
    "## Take Home Messages\n",
    "---\n",
    "1. Proteins can be numerically expressed and used as inputs for machine learning algorithms.\n",
    "2. One-hot encoding creates high-dimensional and sparse representations of protein sequences.\n",
    "3. Training ANNs with gradient based learning:\n",
    "   - Splitting data into train, validation and test sets.\n",
    "   - The training loop consists of a train and validation phase. \n",
    "   - The purpose of validation is to estimate generalization and overfitting.\n",
    "   - The test set \"simulates\" the models performance during inference.\n",
    "   - There are many different optmization algorithms, which lead to different training behaviours.\n",
    "5. The performance of machine learning models can be tracked with proper metrics and visualizations.\n",
    "4. Transforming data to more meaningful spaces improves learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f7ac4-d3e2-4a79-9cd4-8eddbdfee78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
